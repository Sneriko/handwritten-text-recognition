{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"testCycleGANs.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOR01flSe+AvPtstX0lkUr+"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"19BKnBdsgqiW","executionInfo":{"status":"ok","timestamp":1622459145654,"user_tz":-120,"elapsed":319,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}}},"source":["from __future__ import print_function, division\n","import scipy\n","import scipy.misc\n","\n","from tensorflow_addons.layers import InstanceNormalization\n","from tensorflow.keras.datasets import mnist\n","#from tensorflow.keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import UpSampling2D, Conv2D\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n","from sklearn.metrics import mean_squared_error\n","\n","import datetime\n","import sys\n","import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","from glob import glob\n","from tensorflow.keras.models import load_model\n","\n","%matplotlib inline\n","from matplotlib.pylab import rcParams\n","import matplotlib.pyplot as plt\n","rcParams['figure.figsize'] = 30, 5"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-07-15 09:07:35.604850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"]}]},{"cell_type":"code","metadata":{"id":"WXrmY2DkgqiX","executionInfo":{"status":"ok","timestamp":1622459148870,"user_tz":-120,"elapsed":357,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}}},"source":["from tensorflow.keras.layers import Layer, InputSpec\n","from tensorflow.keras import initializers, regularizers, constraints\n","from tensorflow.keras import backend as K\n","\n","\n","class InstanceNormalization(Layer):\n","    \"\"\"Instance normalization layer.\n","\n","    Normalize the activations of the previous layer at each step,\n","    i.e. applies a transformation that maintains the mean activation\n","    close to 0 and the activation standard deviation close to 1.\n","\n","    # Arguments\n","        axis: Integer, the axis that should be normalized\n","            (typically the features axis).\n","            For instance, after a `Conv2D` layer with\n","            `data_format=\"channels_first\"`,\n","            set `axis=1` in `InstanceNormalization`.\n","            Setting `axis=None` will normalize all values in each\n","            instance of the batch.\n","            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n","        epsilon: Small float added to variance to avoid dividing by zero.\n","        center: If True, add offset of `beta` to normalized tensor.\n","            If False, `beta` is ignored.\n","        scale: If True, multiply by `gamma`.\n","            If False, `gamma` is not used.\n","            When the next layer is linear (also e.g. `nn.relu`),\n","            this can be disabled since the scaling\n","            will be done by the next layer.\n","        beta_initializer: Initializer for the beta weight.\n","        gamma_initializer: Initializer for the gamma weight.\n","        beta_regularizer: Optional regularizer for the beta weight.\n","        gamma_regularizer: Optional regularizer for the gamma weight.\n","        beta_constraint: Optional constraint for the beta weight.\n","        gamma_constraint: Optional constraint for the gamma weight.\n","\n","    # Input shape\n","        Arbitrary. Use the keyword argument `input_shape`\n","        (tuple of integers, does not include the samples axis)\n","        when using this layer as the first layer in a Sequential model.\n","\n","    # Output shape\n","        Same shape as input.\n","\n","    # References\n","        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n","        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n","        https://arxiv.org/abs/1607.08022)\n","    \"\"\"\n","    def __init__(self,\n","                 axis=None,\n","                 epsilon=1e-3,\n","                 center=True,\n","                 scale=True,\n","                 beta_initializer='zeros',\n","                 gamma_initializer='ones',\n","                 beta_regularizer=None,\n","                 gamma_regularizer=None,\n","                 beta_constraint=None,\n","                 gamma_constraint=None,\n","                 **kwargs):\n","        super(InstanceNormalization, self).__init__(**kwargs)\n","        self.supports_masking = True\n","        self.axis = axis\n","        self.epsilon = epsilon\n","        self.center = center\n","        self.scale = scale\n","        self.beta_initializer = initializers.get(beta_initializer)\n","        self.gamma_initializer = initializers.get(gamma_initializer)\n","        self.beta_regularizer = regularizers.get(beta_regularizer)\n","        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n","        self.beta_constraint = constraints.get(beta_constraint)\n","        self.gamma_constraint = constraints.get(gamma_constraint)\n","\n","    def build(self, input_shape):\n","        ndim = len(input_shape)\n","        if self.axis == 0:\n","            raise ValueError('Axis cannot be zero')\n","\n","        if (self.axis is not None) and (ndim == 2):\n","            raise ValueError('Cannot specify axis for rank 1 tensor')\n","\n","        self.input_spec = InputSpec(ndim=ndim)\n","\n","        if self.axis is None:\n","            shape = (1,)\n","        else:\n","            shape = (input_shape[self.axis],)\n","\n","        if self.scale:\n","            self.gamma = self.add_weight(shape=shape,\n","                                         name='gamma',\n","                                         initializer=self.gamma_initializer,\n","                                         regularizer=self.gamma_regularizer,\n","                                         constraint=self.gamma_constraint)\n","        else:\n","            self.gamma = None\n","        if self.center:\n","            self.beta = self.add_weight(shape=shape,\n","                                        name='beta',\n","                                        initializer=self.beta_initializer,\n","                                        regularizer=self.beta_regularizer,\n","                                        constraint=self.beta_constraint)\n","        else:\n","            self.beta = None\n","        self.built = True\n","\n","    def call(self, inputs, training=None):\n","        input_shape = K.int_shape(inputs)\n","        reduction_axes = list(range(0, len(input_shape)))\n","\n","        if self.axis is not None:\n","            del reduction_axes[self.axis]\n","\n","        del reduction_axes[0]\n","\n","        mean = K.mean(inputs, reduction_axes, keepdims=True)\n","        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n","        normed = (inputs - mean) / stddev\n","\n","        broadcast_shape = [1] * len(input_shape)\n","        if self.axis is not None:\n","            broadcast_shape[self.axis] = input_shape[self.axis]\n","\n","        if self.scale:\n","            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n","            normed = normed * broadcast_gamma\n","        if self.center:\n","            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n","            normed = normed + broadcast_beta\n","        return normed\n","\n","    def get_config(self):\n","        config = {\n","            'axis': self.axis,\n","            'epsilon': self.epsilon,\n","            'center': self.center,\n","            'scale': self.scale,\n","            'beta_initializer': initializers.serialize(self.beta_initializer),\n","            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n","            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n","            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n","            'beta_constraint': constraints.serialize(self.beta_constraint),\n","            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n","        }\n","        base_config = super(InstanceNormalization, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"DeBa6RwYgqic","executionInfo":{"status":"ok","timestamp":1622461166987,"user_tz":-120,"elapsed":310,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}}},"source":["class DataLoader():\n","    def __init__(self, img_res=(128, 1024)):\n","        self.img_res = img_res\n","\n","    def load_data(self, domain, batch_size=1, is_testing=False, is_random = True):\n","        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n","        path = glob('../../data/1930_census/Gotland/cyclegan/cyclegan_datasets/only_overwritten/%s/*' % data_type)\n","        \n","        path.sort() #patch for demo\n","\n","        if is_random: batch_images = np.random.choice(path, size=batch_size, replace=False)\n","        else: batch_images = path\n","        \n","        imgs = []\n","        for img_path in batch_images:\n","            img = image.load_img(img_path, color_mode='grayscale', target_size=(128, 1024))\n","            img = image.img_to_array(img).astype('float32')\n","            img = img / 255.0\n","            if not is_testing and is_random:\n","\n","                if np.random.random() > 0.5:\n","                    img = np.fliplr(img)\n","\n","              \n","            imgs.append(img)\n","\n","        imgs = np.array(imgs)\n","\n","        return imgs\n","\n","    def load_batch(self, batch_size=1, is_testing=False):\n","        data_typeA = \"trainA\" if not is_testing else \"testA\"\n","        data_typeB = \"trainB\" if not is_testing else \"testB\"\n","        path_A = glob('../../data/1930_census/Gotland/cyclegan/cyclegan_datasets/only_overwritten/%s/*' % (data_typeA))\n","        path_B = glob('../../data/1930_census/Gotland/cyclegan/cyclegan_datasets/only_overwritten/%s/*' % (data_typeB))\n","        \n","        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n","        total_samples = self.n_batches * batch_size\n","\n","        path_A = np.random.choice(path_A, total_samples, replace=False)\n","        path_B = np.random.choice(path_B, total_samples, replace=False)\n","        \n","        \n","\n","        for i in range(self.n_batches):\n","            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n","            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n","            imgs_A, imgs_B = [], []\n","            for img_A, img_B in zip(batch_A, batch_B):\n","                \n","                img_A = image.load_img(img_A, color_mode='grayscale', target_size=(128, 1024))\n","                img_A = image.img_to_array(img_A).astype('float32')\n","                img_A = img_A / 255.0\n","                \n","                img_B = image.load_img(img_B, color_mode='grayscale', target_size=(128, 1024))\n","                img_B = image.img_to_array(img_B).astype('float32')\n","                img_B = img_B / 255.0\n","\n","                if not is_testing and np.random.random() > 0.5:\n","                        img_A = np.fliplr(img_A)\n","                        img_B = np.fliplr(img_B)\n","\n","                imgs_A.append(img_A)\n","                imgs_B.append(img_B)\n","\n","            imgs_A = np.array(imgs_A)\n","            imgs_B = np.array(imgs_B)\n","\n","            yield imgs_A, imgs_B\n","\n","    def load_img(self, path):\n","        img = self.imread(path)\n","        #img.resize(self.img_res)\n","        img = img/255.0\n","        return img[np.newaxis, :, :, :]\n","\n","    def imread(self, path):\n","        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","        u, i = np.unique(np.array(img).flatten(), return_inverse=True)\n","        bg = int(u[np.argmax(np.bincount(i))])\n","\n","        wt, ht, _ = (1024, 128, 1)\n","        h, w = np.asarray(img).shape\n","        f = max((w / wt), (h / ht))\n","\n","        new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n","        img = cv2.resize(img, new_size)\n","\n","        target = np.ones([ht, wt], dtype=np.uint8) * bg\n","        target[0:new_size[1], 0:new_size[0]] = img\n","\n","        target = target.astype(np.float)\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lNnrXawgqic","executionInfo":{"status":"ok","timestamp":1622461171973,"user_tz":-120,"elapsed":586,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}}},"source":["class CycleGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 128\n","        self.img_cols = 1024\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","        # Configure data loader\n","        self.data_loader = DataLoader(img_res=(self.img_rows, self.img_cols))\n","\n","\n","        # Calculate output shape of D (PatchGAN)\n","        patch1 = int(self.img_rows / 2**4)\n","        patch2 = int(self.img_cols / 2**4)\n","        self.disc_patch = (patch1, patch2, 1)\n","\n","        # Number of filters in the first layer of G and D\n","        self.gf = 32\n","        self.df = 64\n","\n","        # Loss weights\n","        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n","        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss (mainly to preserve color consistency)\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminators\n","        self.d_A = self.build_discriminator()\n","        self.d_B = self.build_discriminator()\n","        self.d_A.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","        self.d_B.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        #-------------------------\n","        # Construct Computational\n","        #   Graph of Generators\n","        #-------------------------\n","\n","        # Build the generators\n","        self.g_AB = self.build_generator()\n","        self.g_BA = self.build_generator()\n","\n","        # Input images from both domains\n","        img_A = Input(shape=self.img_shape)\n","        img_B = Input(shape=self.img_shape)\n","        \n","\n","        #img_A = img_A.reshape(1,256, 256,1)\n","        #img_B = img_B.reshape(1,256, 256,1)\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB(img_A)\n","        fake_A = self.g_BA(img_B)\n","        # Translate images back to original domain\n","        reconstr_A = self.g_BA(fake_B)\n","        reconstr_B = self.g_AB(fake_A)\n","        # Identity mapping of images\n","        img_A_id = self.g_BA(img_A)\n","        img_B_id = self.g_AB(img_B)\n","\n","        # For the combined model we will only train the generators\n","        self.d_A.trainable = False\n","        self.d_B.trainable = False\n","\n","        # Discriminators determines validity of translated images\n","        valid_A = self.d_A(fake_A)\n","        valid_B = self.d_B(fake_B)\n","\n","        # Combined model trains generators to fool discriminators\n","        self.combined = Model(inputs=[img_A, img_B],\n","                              outputs=[ valid_A, valid_B,\n","                                        reconstr_A, reconstr_B,\n","                                        img_A_id, img_B_id ])\n","        self.combined.compile(loss=['mse', 'mse',\n","                                    'mae', 'mae',\n","                                    'mae', 'mae'],\n","                            loss_weights=[  1, 1,\n","                                            self.lambda_cycle, self.lambda_cycle,\n","                                            self.lambda_id, self.lambda_id ],\n","                            optimizer=optimizer)\n","\n","    def build_generator(self):\n","        \"\"\"U-Net Generator\"\"\"\n","\n","        def conv2d(layer_input, filters, f_size=4):\n","            \"\"\"Layers used during downsampling\"\"\"\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            d = InstanceNormalization()(d)\n","            return d\n","\n","        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","            \"\"\"Layers used during upsampling\"\"\"\n","            u = UpSampling2D(size=2)(layer_input)\n","            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n","            if dropout_rate:\n","                u = Dropout(dropout_rate)(u)\n","            u = InstanceNormalization()(u)\n","            u = Concatenate()([u, skip_input])\n","            return u\n","\n","        # Image input\n","        d0 = Input(shape=self.img_shape)\n","\n","        # Downsampling\n","        d1 = conv2d(d0, self.gf)\n","        d2 = conv2d(d1, self.gf*2)\n","        d3 = conv2d(d2, self.gf*4)\n","        d4 = conv2d(d3, self.gf*8)\n","\n","        # Upsampling\n","        u1 = deconv2d(d4, d3, self.gf*4)\n","        u2 = deconv2d(u1, d2, self.gf*2)\n","        u3 = deconv2d(u2, d1, self.gf)\n","\n","        u4 = UpSampling2D(size=2)(u3)\n","        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n","        \n","        print(Model(d0,output_img).summary())\n","\n","        return Model(d0, output_img)\n","\n","    def build_discriminator(self):\n","\n","        def d_layer(layer_input, filters, f_size=4, normalization=True):\n","            \"\"\"Discriminator layer\"\"\"\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            if normalization:\n","                d = InstanceNormalization()(d)\n","            return d\n","\n","        img = Input(shape=self.img_shape)\n","\n","        d1 = d_layer(img, self.df, normalization=False)\n","        d2 = d_layer(d1, self.df*2)\n","        d3 = d_layer(d2, self.df*4)\n","        d4 = d_layer(d3, self.df*8)\n","\n","        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=16, sample_interval=10):\n","\n","        start_time = datetime.datetime.now()\n","\n","        # Adversarial loss ground truths\n","        valid = np.ones((batch_size,) + self.disc_patch)\n","        fake = np.zeros((batch_size,) + self.disc_patch)\n","        \n","        g_losses = []\n","        d_losses = []\n","\n","        for epoch in range(epochs):\n","            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n","\n","                # ----------------------\n","                #  Train Discriminators\n","                # ----------------------\n","\n","                # Translate images to opposite domain\n","                imgs_A = imgs_A.reshape(-1,self.img_rows, self.img_cols,1)\n","                imgs_B = imgs_B.reshape(-1,self.img_rows, self.img_cols,1)\n","                fake_B = self.g_AB.predict(imgs_A)\n","                fake_A = self.g_BA.predict(imgs_B)\n","\n","                # Train the discriminators (original images = real / translated = Fake)\n","                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","                # Total disciminator loss\n","                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","\n","                # ------------------\n","                #  Train Generators\n","                # ------------------\n","\n","                # Train the generators\n","                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                                                        [valid, valid,\n","                                                        imgs_A, imgs_B,\n","                                                        imgs_A, imgs_B])\n","\n","                elapsed_time = datetime.datetime.now() - start_time\n","\n","                # Plot the progress\n","                if batch_i % 10 == 0:\n","                    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n","                                                                        % ( epoch+1, epochs,\n","                                                                            batch_i+1, self.data_loader.n_batches,\n","                                                                            d_loss[0], 100*d_loss[1],\n","                                                                            g_loss[0],\n","                                                                            np.mean(g_loss[1:3]),\n","                                                                            np.mean(g_loss[3:5]),\n","                                                                            np.mean(g_loss[5:6]),\n","                                                                            elapsed_time))\n","                g_losses.append(g_loss[0])\n","                d_losses.append(d_loss[0])\n","\n","            # If at save interval => save generated image samples\n","            if (epoch % sample_interval) == 0 and (epoch > 100):\n","                self.sample_images(epoch, batch_i)\n","                if epoch % (2*sample_interval) == 0:\n","                    self.d_A.save('../../output/cycleGAN_models/overwritten/%d_d_A.h5' % epoch)\n","                    self.d_B.save('../../output/cycleGAN_models/overwritten/%d_d_B.h5' % epoch)\n","                    self.g_AB.save('../../output/cycleGAN_models/overwritten/%d_g_AB.h5' % epoch)\n","                    self.g_BA.save('../../output/cycleGAN_models/overwritten/%d_g_BA.h5' % epoch)\n","                \n","        return d_losses, g_losses\n","\n","    def sample_images(self, epoch, batch_i):\n","        os.makedirs('gan_images/', exist_ok=True)\n","        r, c = 2, 3\n","\n","        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","\n","        imgs_A = imgs_A.reshape(-1,self.img_rows, self.img_cols,1)\n","        imgs_B = imgs_B.reshape(-1,self.img_rows, self.img_cols,1)\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","        # Translate back to original domain\n","        reconstr_A = self.g_BA.predict(fake_B)\n","        reconstr_B = self.g_AB.predict(fake_A)\n","        \n","        gen_imgs = np.concatenate([imgs_A[:,:,:,0], fake_B[:,:,:,0], reconstr_A[:,:,:,0], imgs_B[:,:,:,0], fake_A[:,:,:,0], reconstr_B[:,:,:,0]])\n","\n","        titles = ['Original', 'Translated', 'Reconstructed']\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt], cmap = 'gray')\n","                axs[i, j].set_title(titles[j])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        \n","        fig.savefig(\"../../output/cycleGAN_models/overwritten/%d_%d.png\" % (epoch, batch_i), dpi = 800)\n","        plt.close()\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":["3] time: 3:49:46.374988 \n","[Epoch 121/301] [Batch 41/79] [D loss: 0.209863, acc:  62%] [G loss: 1.613835, adv: 0.521374, recon: 0.025827, id: 0.030238] time: 3:50:00.897000 \n","[Epoch 121/301] [Batch 51/79] [D loss: 0.198555, acc:  65%] [G loss: 2.450299, adv: 0.872158, recon: 0.032457, id: 0.031646] time: 3:50:15.416412 \n","[Epoch 121/301] [Batch 61/79] [D loss: 0.218741, acc:  53%] [G loss: 2.079559, adv: 0.702376, recon: 0.030848, id: 0.023353] time: 3:50:29.897600 \n","[Epoch 121/301] [Batch 71/79] [D loss: 0.205552, acc:  62%] [G loss: 1.577268, adv: 0.529440, recon: 0.023644, id: 0.022915] time: 3:50:44.442704 \n","[Epoch 122/301] [Batch 1/79] [D loss: 0.235604, acc:  57%] [G loss: 1.446675, adv: 0.455337, recon: 0.024478, id: 0.021618] time: 3:51:01.812761 \n","[Epoch 122/301] [Batch 11/79] [D loss: 0.227010, acc:  60%] [G loss: 1.710016, adv: 0.493592, recon: 0.032557, id: 0.037602] time: 3:51:16.279692 \n","[Epoch 122/301] [Batch 21/79] [D loss: 0.192657, acc:  62%] [G loss: 1.978341, adv: 0.606396, recon: 0.035190, id: 0.025884] time: 3:51:30.682314 \n","[Epoch 122/301] [Batch 31/79] [D loss: 0.275717, acc:  41%] [G loss: 1.785983, adv: 0.616284, recon: 0.025295, id: 0.024794] time: 3:51:45.155753 \n","[Epoch 122/301] [Batch 41/79] [D loss: 0.238807, acc:  52%] [G loss: 1.704697, adv: 0.579414, recon: 0.024856, id: 0.026491] time: 3:51:59.652887 \n","[Epoch 122/301] [Batch 51/79] [D loss: 0.227625, acc:  54%] [G loss: 1.721436, adv: 0.598622, recon: 0.023828, id: 0.026166] time: 3:52:14.083635 \n","[Epoch 122/301] [Batch 61/79] [D loss: 0.202814, acc:  61%] [G loss: 1.827217, adv: 0.642176, recon: 0.024681, id: 0.026384] time: 3:52:28.505873 \n","[Epoch 122/301] [Batch 71/79] [D loss: 0.222058, acc:  61%] [G loss: 1.729321, adv: 0.591296, recon: 0.024972, id: 0.025224] time: 3:52:43.013096 \n","[Epoch 123/301] [Batch 1/79] [D loss: 0.250227, acc:  51%] [G loss: 2.257268, adv: 0.780606, recon: 0.032026, id: 0.024502] time: 3:52:56.005311 \n","[Epoch 123/301] [Batch 11/79] [D loss: 0.281743, acc:  41%] [G loss: 1.479336, adv: 0.481571, recon: 0.023346, id: 0.024860] time: 3:53:10.409451 \n","[Epoch 123/301] [Batch 21/79] [D loss: 0.211279, acc:  56%] [G loss: 1.881945, adv: 0.666972, recon: 0.024585, id: 0.027026] time: 3:53:24.825861 \n","[Epoch 123/301] [Batch 31/79] [D loss: 0.184482, acc:  64%] [G loss: 1.992714, adv: 0.729252, recon: 0.024110, id: 0.024119] time: 3:53:39.314867 \n","[Epoch 123/301] [Batch 41/79] [D loss: 0.162090, acc:  73%] [G loss: 2.025341, adv: 0.725644, recon: 0.025715, id: 0.039313] time: 3:53:53.765475 \n","[Epoch 123/301] [Batch 51/79] [D loss: 0.217097, acc:  57%] [G loss: 1.743790, adv: 0.577145, recon: 0.026910, id: 0.027438] time: 3:54:08.214451 \n","[Epoch 123/301] [Batch 61/79] [D loss: 0.342045, acc:  38%] [G loss: 1.266778, adv: 0.347045, recon: 0.026165, id: 0.023172] time: 3:54:22.730982 \n","[Epoch 123/301] [Batch 71/79] [D loss: 0.210842, acc:  57%] [G loss: 1.765665, adv: 0.615606, recon: 0.024263, id: 0.026798] time: 3:54:37.209543 \n","[Epoch 124/301] [Batch 1/79] [D loss: 0.224633, acc:  57%] [G loss: 1.984980, adv: 0.708797, recon: 0.025494, id: 0.031117] time: 3:54:50.264590 \n","[Epoch 124/301] [Batch 11/79] [D loss: 0.180575, acc:  67%] [G loss: 1.934639, adv: 0.704429, recon: 0.023598, id: 0.031617] time: 3:55:05.090968 \n","[Epoch 124/301] [Batch 21/79] [D loss: 0.196977, acc:  62%] [G loss: 2.064551, adv: 0.666438, recon: 0.032871, id: 0.033308] time: 3:55:19.709135 \n","[Epoch 124/301] [Batch 31/79] [D loss: 0.222613, acc:  62%] [G loss: 1.614943, adv: 0.518180, recon: 0.026241, id: 0.028977] time: 3:55:34.238100 \n","[Epoch 124/301] [Batch 41/79] [D loss: 0.299827, acc:  42%] [G loss: 1.773082, adv: 0.632662, recon: 0.022959, id: 0.025736] time: 3:55:48.717187 \n","[Epoch 124/301] [Batch 51/79] [D loss: 0.220359, acc:  58%] [G loss: 1.688668, adv: 0.573921, recon: 0.024325, id: 0.032794] time: 3:56:03.208460 \n","[Epoch 124/301] [Batch 61/79] [D loss: 0.282707, acc:  40%] [G loss: 1.730156, adv: 0.583039, recon: 0.025718, id: 0.024397] time: 3:56:17.668028 \n","[Epoch 124/301] [Batch 71/79] [D loss: 0.171708, acc:  73%] [G loss: 1.722178, adv: 0.607320, recon: 0.023014, id: 0.024992] time: 3:56:32.155347 \n","[Epoch 125/301] [Batch 1/79] [D loss: 0.224227, acc:  57%] [G loss: 1.733909, adv: 0.628731, recon: 0.021417, id: 0.026431] time: 3:56:45.211751 \n","[Epoch 125/301] [Batch 11/79] [D loss: 0.265084, acc:  47%] [G loss: 1.590527, adv: 0.530570, recon: 0.023979, id: 0.024600] time: 3:56:59.656190 \n","[Epoch 125/301] [Batch 21/79] [D loss: 0.176989, acc:  69%] [G loss: 1.827972, adv: 0.643904, recon: 0.024564, id: 0.023485] time: 3:57:14.136273 \n","[Epoch 125/301] [Batch 31/79] [D loss: 0.186845, acc:  69%] [G loss: 1.865038, adv: 0.663572, recon: 0.024450, id: 0.029161] time: 3:57:28.629142 \n","[Epoch 125/301] [Batch 41/79] [D loss: 0.240017, acc:  59%] [G loss: 1.656611, adv: 0.556475, recon: 0.024837, id: 0.025168] time: 3:57:43.146251 \n","[Epoch 125/301] [Batch 51/79] [D loss: 0.296080, acc:  38%] [G loss: 1.761871, adv: 0.638435, recon: 0.021955, id: 0.026008] time: 3:57:57.624363 \n","[Epoch 125/301] [Batch 61/79] [D loss: 0.205372, acc:  64%] [G loss: 1.506267, adv: 0.480469, recon: 0.024618, id: 0.028348] time: 3:58:12.177763 \n","[Epoch 125/301] [Batch 71/79] [D loss: 0.250955, acc:  51%] [G loss: 1.749208, adv: 0.619178, recon: 0.022968, id: 0.027757] time: 3:58:26.671675 \n","[Epoch 126/301] [Batch 1/79] [D loss: 0.255103, acc:  52%] [G loss: 2.572102, adv: 0.926362, recon: 0.033186, id: 0.024682] time: 3:58:39.754357 \n","[Epoch 126/301] [Batch 11/79] [D loss: 0.288445, acc:  44%] [G loss: 1.634044, adv: 0.586300, recon: 0.021075, id: 0.020453] time: 3:58:54.207119 \n","[Epoch 126/301] [Batch 21/79] [D loss: 0.273658, acc:  52%] [G loss: 2.476817, adv: 0.935928, recon: 0.027404, id: 0.026894] time: 3:59:08.700324 \n","[Epoch 126/301] [Batch 31/79] [D loss: 0.307463, acc:  35%] [G loss: 1.573876, adv: 0.535076, recon: 0.023063, id: 0.020623] time: 3:59:23.198301 \n","[Epoch 126/301] [Batch 41/79] [D loss: 0.189477, acc:  67%] [G loss: 2.001468, adv: 0.714868, recon: 0.025824, id: 0.027925] time: 3:59:37.684273 \n","[Epoch 126/301] [Batch 51/79] [D loss: 0.185354, acc:  68%] [G loss: 2.045533, adv: 0.728307, recon: 0.026958, id: 0.024755] time: 3:59:52.179007 \n","[Epoch 126/301] [Batch 61/79] [D loss: 0.157610, acc:  77%] [G loss: 2.233472, adv: 0.808401, recon: 0.028101, id: 0.031201] time: 4:00:06.658579 \n","[Epoch 126/301] [Batch 71/79] [D loss: 0.222216, acc:  64%] [G loss: 1.659064, adv: 0.551359, recon: 0.025346, id: 0.022524] time: 4:00:21.138255 \n","[Epoch 127/301] [Batch 1/79] [D loss: 0.167582, acc:  76%] [G loss: 2.301804, adv: 0.801550, recon: 0.032091, id: 0.027173] time: 4:00:34.181968 \n","[Epoch 127/301] [Batch 11/79] [D loss: 0.219146, acc:  62%] [G loss: 1.871965, adv: 0.579532, recon: 0.032818, id: 0.025622] time: 4:00:48.635273 \n","[Epoch 127/301] [Batch 21/79] [D loss: 0.174312, acc:  73%] [G loss: 1.906889, adv: 0.629283, recon: 0.029926, id: 0.027118] time: 4:01:03.066667 \n","[Epoch 127/301] [Batch 31/79] [D loss: 0.263665, acc:  49%] [G loss: 1.802382, adv: 0.631889, recon: 0.024463, id: 0.027272] time: 4:01:17.487785 \n","[Epoch 127/301] [Batch 41/79] [D loss: 0.181064, acc:  70%] [G loss: 1.761554, adv: 0.600255, recon: 0.025860, id: 0.021905] time: 4:01:31.924668 \n","[Epoch 127/301] [Batch 51/79] [D loss: 0.214147, acc:  59%] [G loss: 1.527080, adv: 0.504583, recon: 0.023610, id: 0.022777] time: 4:01:46.384381 \n","[Epoch 127/301] [Batch 61/79] [D loss: 0.209049, acc:  54%] [G loss: 2.126706, adv: 0.646360, recon: 0.037939, id: 0.036563] time: 4:02:00.828228 \n","[Epoch 127/301] [Batch 71/79] [D loss: 0.223331, acc:  67%] [G loss: 1.950918, adv: 0.697928, recon: 0.025511, id: 0.022597] time: 4:02:15.320988 \n","[Epoch 128/301] [Batch 1/79] [D loss: 0.169804, acc:  72%] [G loss: 1.869349, adv: 0.656079, recon: 0.025608, id: 0.022616] time: 4:02:28.363587 \n","[Epoch 128/301] [Batch 11/79] [D loss: 0.177810, acc:  75%] [G loss: 2.180085, adv: 0.799043, recon: 0.026348, id: 0.032151] time: 4:02:42.837195 \n","[Epoch 128/301] [Batch 21/79] [D loss: 0.150484, acc:  81%] [G loss: 2.245220, adv: 0.854099, recon: 0.024712, id: 0.023040] time: 4:02:57.343808 \n","[Epoch 128/301] [Batch 31/79] [D loss: 0.228749, acc:  63%] [G loss: 2.215544, adv: 0.841091, recon: 0.024456, id: 0.021917] time: 4:03:11.812897 \n","[Epoch 128/301] [Batch 41/79] [D loss: 0.092990, acc:  92%] [G loss: 2.258971, adv: 0.842236, recon: 0.026503, id: 0.020796] time: 4:03:26.269798 \n","[Epoch 128/301] [Batch 51/79] [D loss: 0.209560, acc:  68%] [G loss: 2.313549, adv: 0.868607, recon: 0.026609, id: 0.021792] time: 4:03:40.754433 \n","[Epoch 128/301] [Batch 61/79] [D loss: 0.173568, acc:  74%] [G loss: 2.319559, adv: 0.894170, recon: 0.024129, id: 0.024187] time: 4:03:55.199959 \n","[Epoch 128/301] [Batch 71/79] [D loss: 0.144450, acc:  80%] [G loss: 2.300450, adv: 0.799995, recon: 0.032081, id: 0.029735] time: 4:04:09.696499 \n","[Epoch 129/301] [Batch 1/79] [D loss: 0.172491, acc:  79%] [G loss: 2.145998, adv: 0.755963, recon: 0.028823, id: 0.034256] time: 4:04:22.809945 \n","[Epoch 129/301] [Batch 11/79] [D loss: 0.294106, acc:  56%] [G loss: 1.517995, adv: 0.532234, recon: 0.020516, id: 0.024269] time: 4:04:37.275645 \n","[Epoch 129/301] [Batch 21/79] [D loss: 0.204021, acc:  64%] [G loss: 1.755420, adv: 0.579119, recon: 0.027283, id: 0.026200] time: 4:04:51.813261 \n","[Epoch 129/301] [Batch 31/79] [D loss: 0.190251, acc:  69%] [G loss: 2.250965, adv: 0.852381, recon: 0.024891, id: 0.022028] time: 4:05:06.286017 \n","[Epoch 129/301] [Batch 41/79] [D loss: 0.138658, acc:  82%] [G loss: 2.360479, adv: 0.868007, recon: 0.028602, id: 0.026999] time: 4:05:20.764715 \n","[Epoch 129/301] [Batch 51/79] [D loss: 0.156501, acc:  79%] [G loss: 2.144060, adv: 0.780991, recon: 0.026683, id: 0.027109] time: 4:05:35.305546 \n","[Epoch 129/301] [Batch 61/79] [D loss: 0.178254, acc:  76%] [G loss: 2.537695, adv: 0.957643, recon: 0.028231, id: 0.030952] time: 4:05:49.900117 \n","[Epoch 129/301] [Batch 71/79] [D loss: 0.118431, acc:  85%] [G loss: 2.185374, adv: 0.807548, recon: 0.026190, id: 0.023782] time: 4:06:04.692486 \n","[Epoch 130/301] [Batch 1/79] [D loss: 0.227787, acc:  67%] [G loss: 1.843479, adv: 0.637784, recon: 0.026006, id: 0.025029] time: 4:06:17.886625 \n","[Epoch 130/301] [Batch 11/79] [D loss: 0.141925, acc:  81%] [G loss: 1.921733, adv: 0.712140, recon: 0.022501, id: 0.028202] time: 4:06:32.416109 \n","[Epoch 130/301] [Batch 21/79] [D loss: 0.183399, acc:  77%] [G loss: 1.905554, adv: 0.650267, recon: 0.027895, id: 0.026387] time: 4:06:46.939080 \n","[Epoch 130/301] [Batch 31/79] [D loss: 0.203161, acc:  68%] [G loss: 2.176481, adv: 0.804152, recon: 0.026031, id: 0.025332] time: 4:07:01.405797 \n","[Epoch 130/301] [Batch 41/79] [D loss: 0.122713, acc:  85%] [G loss: 2.007585, adv: 0.734029, recon: 0.024920, id: 0.019482] time: 4:07:15.956225 \n","[Epoch 130/301] [Batch 51/79] [D loss: 0.196318, acc:  71%] [G loss: 2.431662, adv: 0.788256, recon: 0.039222, id: 0.026631] time: 4:07:30.439381 \n","[Epoch 130/301] [Batch 61/79] [D loss: 0.120238, acc:  88%] [G loss: 2.303735, adv: 0.856071, recon: 0.026991, id: 0.027978] time: 4:07:45.016992 \n","[Epoch 130/301] [Batch 71/79] [D loss: 0.193719, acc:  68%] [G loss: 2.103335, adv: 0.759219, recon: 0.026614, id: 0.028165] time: 4:07:59.492773 \n","[Epoch 131/301] [Batch 1/79] [D loss: 0.187539, acc:  70%] [G loss: 2.134451, adv: 0.767995, recon: 0.027293, id: 0.029114] time: 4:08:12.578992 \n","[Epoch 131/301] [Batch 11/79] [D loss: 0.195629, acc:  70%] [G loss: 2.388087, adv: 0.842089, recon: 0.032249, id: 0.032901] time: 4:08:27.061717 \n","[Epoch 131/301] [Batch 21/79] [D loss: 0.114374, acc:  90%] [G loss: 2.300649, adv: 0.866930, recon: 0.025951, id: 0.025513] time: 4:08:41.502840 \n","[Epoch 131/301] [Batch 31/79] [D loss: 0.247880, acc:  63%] [G loss: 2.318638, adv: 0.689850, recon: 0.043299, id: 0.030564] time: 4:08:55.979830 \n","[Epoch 131/301] [Batch 41/79] [D loss: 0.236899, acc:  64%] [G loss: 2.776922, adv: 1.060313, recon: 0.029941, id: 0.029643] time: 4:09:10.493340 \n","[Epoch 131/301] [Batch 51/79] [D loss: 0.183184, acc:  72%] [G loss: 2.181906, adv: 0.780101, recon: 0.028495, id: 0.028150] time: 4:09:24.923343 \n","[Epoch 131/301] [Batch 61/79] [D loss: 0.192890, acc:  70%] [G loss: 1.679204, adv: 0.584732, recon: 0.023278, id: 0.020765] time: 4:09:39.324847 \n","[Epoch 131/301] [Batch 71/79] [D loss: 0.214489, acc:  68%] [G loss: 2.695632, adv: 1.038060, recon: 0.028176, id: 0.025366] time: 4:09:53.829300 \n","[Epoch 132/301] [Batch 1/79] [D loss: 0.148705, acc:  78%] [G loss: 2.259413, adv: 0.864852, recon: 0.024330, id: 0.021877] time: 4:10:06.891180 \n","[Epoch 132/301] [Batch 11/79] [D loss: 0.196883, acc:  71%] [G loss: 1.659296, adv: 0.559353, recon: 0.024454, id: 0.028759] time: 4:10:21.447403 \n","[Epoch 132/301] [Batch 21/79] [D loss: 0.167882, acc:  77%] [G loss: 2.411542, adv: 0.883937, recon: 0.029392, id: 0.028788] time: 4:10:35.901592 \n","[Epoch 132/301] [Batch 31/79] [D loss: 0.106822, acc:  91%] [G loss: 3.291562, adv: 1.331671, recon: 0.029075, id: 0.024784] time: 4:10:50.364820 \n","[Epoch 132/301] [Batch 41/79] [D loss: 0.193497, acc:  74%] [G loss: 2.502295, adv: 0.909610, recon: 0.031250, id: 0.029954] time: 4:11:04.897128 \n","[Epoch 132/301] [Batch 51/79] [D loss: 0.107578, acc:  91%] [G loss: 2.492252, adv: 0.911349, recon: 0.030960, id: 0.025655] time: 4:11:19.348236 \n","[Epoch 132/301] [Batch 61/79] [D loss: 0.132701, acc:  82%] [G loss: 2.579788, adv: 0.904826, recon: 0.035957, id: 0.023008] time: 4:11:33.819531 \n","[Epoch 132/301] [Batch 71/79] [D loss: 0.267689, acc:  59%] [G loss: 2.366803, adv: 0.819118, recon: 0.033401, id: 0.032783] time: 4:11:48.306924 \n","[Epoch 133/301] [Batch 1/79] [D loss: 0.134733, acc:  85%] [G loss: 2.126614, adv: 0.786388, recon: 0.025128, id: 0.025813] time: 4:12:01.316809 \n","[Epoch 133/301] [Batch 11/79] [D loss: 0.125169, acc:  85%] [G loss: 2.656348, adv: 0.978298, recon: 0.032125, id: 0.033419] time: 4:12:15.788737 \n","[Epoch 133/301] [Batch 21/79] [D loss: 0.255092, acc:  66%] [G loss: 2.472413, adv: 0.903634, recon: 0.030406, id: 0.033890] time: 4:12:30.228581 \n","[Epoch 133/301] [Batch 31/79] [D loss: 0.164615, acc:  80%] [G loss: 3.153720, adv: 1.219028, recon: 0.032512, id: 0.035616] time: 4:12:44.720328 \n","[Epoch 133/301] [Batch 41/79] [D loss: 0.075701, acc:  96%] [G loss: 2.512225, adv: 0.934007, recon: 0.029766, id: 0.024853] time: 4:12:59.255543 \n","[Epoch 133/301] [Batch 51/79] [D loss: 0.117015, acc:  86%] [G loss: 2.490287, adv: 0.919886, recon: 0.029622, id: 0.033448] time: 4:13:13.750064 \n","[Epoch 133/301] [Batch 61/79] [D loss: 0.164743, acc:  77%] [G loss: 2.269046, adv: 0.793725, recon: 0.031095, id: 0.028423] time: 4:13:28.256402 \n","[Epoch 133/301] [Batch 71/79] [D loss: 0.181841, acc:  75%] [G loss: 2.740812, adv: 1.070040, recon: 0.027403, id: 0.031948] time: 4:13:42.804021 \n","[Epoch 134/301] [Batch 1/79] [D loss: 0.103499, acc:  89%] [G loss: 2.727631, adv: 1.034510, recon: 0.030210, id: 0.035630] time: 4:13:55.814683 \n","[Epoch 134/301] [Batch 11/79] [D loss: 0.177133, acc:  76%] [G loss: 2.291866, adv: 0.853537, recon: 0.026882, id: 0.022111] time: 4:14:10.378283 \n","[Epoch 134/301] [Batch 21/79] [D loss: 0.110905, acc:  89%] [G loss: 2.692933, adv: 1.028931, recon: 0.029061, id: 0.025484] time: 4:14:24.875999 \n","[Epoch 134/301] [Batch 31/79] [D loss: 0.228104, acc:  50%] [G loss: 1.738706, adv: 0.566195, recon: 0.027719, id: 0.021502] time: 4:14:39.317858 \n","[Epoch 134/301] [Batch 41/79] [D loss: 0.238031, acc:  58%] [G loss: 2.030272, adv: 0.688167, recon: 0.030132, id: 0.028214] time: 4:14:53.766847 \n","[Epoch 134/301] [Batch 51/79] [D loss: 0.176594, acc:  76%] [G loss: 2.036109, adv: 0.684832, recon: 0.031236, id: 0.023604] time: 4:15:08.248480 \n","[Epoch 134/301] [Batch 61/79] [D loss: 0.180445, acc:  73%] [G loss: 2.147192, adv: 0.756670, recon: 0.029561, id: 0.021654] time: 4:15:22.782443 \n","[Epoch 134/301] [Batch 71/79] [D loss: 0.250869, acc:  51%] [G loss: 1.987428, adv: 0.645355, recon: 0.032168, id: 0.027903] time: 4:15:37.242551 \n","[Epoch 135/301] [Batch 1/79] [D loss: 0.265307, acc:  50%] [G loss: 1.759423, adv: 0.595903, recon: 0.026036, id: 0.025966] time: 4:15:50.256133 \n","[Epoch 135/301] [Batch 11/79] [D loss: 0.187222, acc:  72%] [G loss: 2.139501, adv: 0.745481, recon: 0.030209, id: 0.019271] time: 4:16:04.814144 \n","[Epoch 135/301] [Batch 21/79] [D loss: 0.165194, acc:  80%] [G loss: 2.264821, adv: 0.816279, recon: 0.028959, id: 0.029977] time: 4:16:19.299917 \n","[Epoch 135/301] [Batch 31/79] [D loss: 0.136492, acc:  85%] [G loss: 2.773252, adv: 1.098685, recon: 0.026296, id: 0.029714] time: 4:16:33.736516 \n","[Epoch 135/301] [Batch 41/79] [D loss: 0.215227, acc:  66%] [G loss: 3.239126, adv: 1.286559, recon: 0.030561, id: 0.030942] time: 4:16:48.277619 \n","[Epoch 135/301] [Batch 51/79] [D loss: 0.345699, acc:  56%] [G loss: 2.718753, adv: 0.829736, recon: 0.049810, id: 0.035728] time: 4:17:02.799412 \n","[Epoch 135/301] [Batch 61/79] [D loss: 0.109081, acc:  91%] [G loss: 2.955827, adv: 1.146751, recon: 0.031223, id: 0.020532] time: 4:17:18.001830 \n","[Epoch 135/301] [Batch 71/79] [D loss: 0.165427, acc:  80%] [G loss: 2.638291, adv: 0.958520, recon: 0.033053, id: 0.028261] time: 4:17:32.566566 \n","[Epoch 136/301] [Batch 1/79] [D loss: 0.095868, acc:  90%] [G loss: 3.155316, adv: 1.158240, recon: 0.038751, id: 0.037666] time: 4:17:45.696312 \n","[Epoch 136/301] [Batch 11/79] [D loss: 0.101095, acc:  92%] [G loss: 2.885596, adv: 1.097416, recon: 0.031790, id: 0.025273] time: 4:18:00.234238 \n","[Epoch 136/301] [Batch 21/79] [D loss: 0.089482, acc:  96%] [G loss: 3.099082, adv: 1.162069, recon: 0.036235, id: 0.023810] time: 4:18:14.839498 \n","[Epoch 136/301] [Batch 31/79] [D loss: 0.181584, acc:  73%] [G loss: 2.841060, adv: 1.099625, recon: 0.029476, id: 0.030781] time: 4:18:29.394656 \n","[Epoch 136/301] [Batch 41/79] [D loss: 0.154221, acc:  78%] [G loss: 2.842867, adv: 1.094222, recon: 0.029911, id: 0.032607] time: 4:18:43.865469 \n","[Epoch 136/301] [Batch 51/79] [D loss: 0.137560, acc:  83%] [G loss: 3.000622, adv: 1.181035, recon: 0.028884, id: 0.039800] time: 4:18:58.329434 \n","[Epoch 136/301] [Batch 61/79] [D loss: 0.106990, acc:  87%] [G loss: 2.233654, adv: 0.827771, recon: 0.026419, id: 0.026320] time: 4:19:12.830692 \n","[Epoch 136/301] [Batch 71/79] [D loss: 0.122377, acc:  87%] [G loss: 2.880580, adv: 1.090151, recon: 0.032101, id: 0.025496] time: 4:19:27.326927 \n","[Epoch 137/301] [Batch 1/79] [D loss: 0.113782, acc:  86%] [G loss: 2.819749, adv: 1.046645, recon: 0.033439, id: 0.029386] time: 4:19:40.419854 \n","[Epoch 137/301] [Batch 11/79] [D loss: 0.071673, acc:  95%] [G loss: 2.893016, adv: 1.050050, recon: 0.036990, id: 0.029860] time: 4:19:54.898000 \n","[Epoch 137/301] [Batch 21/79] [D loss: 0.169138, acc:  77%] [G loss: 3.036736, adv: 1.155066, recon: 0.033516, id: 0.027147] time: 4:20:09.364497 \n","[Epoch 137/301] [Batch 31/79] [D loss: 0.095311, acc:  90%] [G loss: 2.867434, adv: 1.138021, recon: 0.027185, id: 0.024406] time: 4:20:23.842067 \n","[Epoch 137/301] [Batch 41/79] [D loss: 0.154006, acc:  80%] [G loss: 2.418619, adv: 0.877851, recon: 0.030276, id: 0.028825] time: 4:20:38.295078 \n","[Epoch 137/301] [Batch 51/79] [D loss: 0.199025, acc:  73%] [G loss: 2.330918, adv: 0.806434, recon: 0.033423, id: 0.025699] time: 4:20:52.817502 \n","[Epoch 137/301] [Batch 61/79] [D loss: 0.207374, acc:  68%] [G loss: 2.996464, adv: 1.081573, recon: 0.038415, id: 0.032793] time: 4:21:07.306187 \n","[Epoch 137/301] [Batch 71/79] [D loss: 0.206838, acc:  68%] [G loss: 2.623244, adv: 0.974044, recon: 0.031523, id: 0.024500] time: 4:21:21.817188 \n","[Epoch 138/301] [Batch 1/79] [D loss: 0.169446, acc:  72%] [G loss: 2.539969, adv: 0.931459, recon: 0.030768, id: 0.029969] time: 4:21:34.845393 \n","[Epoch 138/301] [Batch 11/79] [D loss: 0.101367, acc:  90%] [G loss: 3.225904, adv: 1.254107, recon: 0.032717, id: 0.034524] time: 4:24:33.273662 \n","[Epoch 138/301] [Batch 21/79] [D loss: 0.222173, acc:  65%] [G loss: 2.460850, adv: 0.794371, recon: 0.039879, id: 0.031299] time: 4:28:41.276626 \n","[Epoch 138/301] [Batch 31/79] [D loss: 0.056351, acc:  97%] [G loss: 2.772805, adv: 1.023448, recon: 0.033297, id: 0.032269] time: 4:38:24.176888 \n"]}],"source":["gan = CycleGAN()\n","history = gan.train(epochs=301,batch_size=16,sample_interval=30)"]},{"cell_type":"code","metadata":{"id":"sPaaYuupumJX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622461184191,"user_tz":-120,"elapsed":1430,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}},"outputId":"45de0b14-7e6e-4deb-a312-d96154ff2123"},"source":["\n","\n","g_AB = load_model('/home/erik/Riksarkivet/Projects/handwritten-text-recognition/output/cycleGAN_models/300_g_AB.h5',custom_objects={'InstanceNormalization': InstanceNormalization()})\n","#g_BA = load_model('/home/erik/Riksarkivet/Projects/handwritten-text-recognition/output/cycleGAN_models/overwritten/180_g_BA.h5',custom_objects={'InstanceNormalization': InstanceNormalization()})\n","\n","#data_loader = DataLoader()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-07-15 08:28:07.445939: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-07-15 08:28:07.446496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n","pciBusID: 0000:02:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n","coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n","2021-07-15 08:28:07.446538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-15 08:28:07.446569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n","2021-07-15 08:28:07.446582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","2021-07-15 08:28:07.446594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n","2021-07-15 08:28:07.446611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n","2021-07-15 08:28:07.446627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n","2021-07-15 08:28:07.446639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n","2021-07-15 08:28:07.446651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n","2021-07-15 08:28:07.447402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n","2021-07-15 08:28:07.447637: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-07-15 08:28:07.448047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n","pciBusID: 0000:02:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n","coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n","2021-07-15 08:28:07.448070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-15 08:28:07.448094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n","2021-07-15 08:28:07.448106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","2021-07-15 08:28:07.448118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n","2021-07-15 08:28:07.448129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n","2021-07-15 08:28:07.448141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n","2021-07-15 08:28:07.448153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n","2021-07-15 08:28:07.448165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n","2021-07-15 08:28:07.448769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n","2021-07-15 08:28:07.448797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-07-15 08:28:07.448803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n","2021-07-15 08:28:07.448809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n","2021-07-15 08:28:07.449462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 454 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:02:00.0, compute capability: 7.5)\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cfpctb9yqsam","executionInfo":{"status":"ok","timestamp":1622181771549,"user_tz":-120,"elapsed":93965,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}},"outputId":"bfda77a9-e82a-44c2-d294-698b7ed714ac"},"source":["r, c = 2, 3\n","\n","imgs_A_batch = data_loader.load_data(domain=\"A\", is_testing=True, is_random=False, batch_size=40)\n","#imgs_B_batch = data_loader.load_data(domain=\"B\", is_testing=True, is_random=False, batch_size=5)\n","\n","img_no = 1\n","\n","for imgs_A, imgs_B in zip(imgs_A_batch, imgs_B_batch):\n","\n","  print(img_no)\n","  imgs_A = imgs_A.reshape(-1,128, 1024,1)\n","  imgs_B = imgs_B.reshape(-1,128, 1024,1)\n","\n","  # Translate images to the other domain\n","  fake_B = g_AB.predict(imgs_A)\n","  fake_A = g_BA.predict(imgs_B)\n","  # Translate back to original domain\n","  reconstr_A = g_BA.predict(fake_B)\n","  reconstr_B = g_AB.predict(fake_A)\n","\n","  gen_imgs = np.concatenate([imgs_A[:,:,:,0], fake_B[:,:,:,0], reconstr_A[:,:,:,0], imgs_B[:,:,:,0], fake_A[:,:,:,0], reconstr_B[:,:,:,0]])\n","\n","  titles = ['Original', 'Translated', 'Reconstructed']\n","  fig, axs = plt.subplots(r, c)\n","  cnt = 0\n","  for i in range(r):\n","      for j in range(c):\n","          axs[i,j].imshow(gen_imgs[cnt], cmap = 'gray')\n","          axs[i, j].set_title(titles[j])\n","          axs[i,j].axis('off')\n","          cnt += 1\n","\n","  fig.savefig(\"/content/sample_data/test_%d.png\" % img_no, dpi = 800)\n","  fig.clf()\n","  pylab.close()\n","\n","  img_no += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2coQIlUmBzD6","executionInfo":{"status":"ok","timestamp":1622445827395,"user_tz":-120,"elapsed":210,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}},"outputId":"a859c257-626a-400b-df01-567a54eb67e0"},"source":["imgs_A_batch = data_loader.load_data(domain=\"A\", is_testing=True, is_random=False)\n","\n","print(len(imgs_A_batch))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["40\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0G6gufhzgnrC","tags":[]},"source":["\n","from PIL import Image as im\n","\n","import tensorflow as tf\n","physical_devices = tf.config.list_physical_devices('GPU') \n","tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","g_AB = load_model('/home/erik/Riksarkivet/Projects/handwritten-text-recognition/output/cycleGAN_models/overwritten/incomplete/240_g_AB.h5',custom_objects={'InstanceNormalization': InstanceNormalization()})\n","\n","data_loader = DataLoader()\n","\n","imgs = data_loader.load_data(domain=\"A\", is_testing=True, is_random=False, batch_size=1)\n","\n","for i, img in enumerate(imgs):\n","  \n","  img_p = img.reshape(-1, 128, 1024, 1)\n","  \n","  img_p = g_AB.predict(img_p)\n","\n","  print('hej')\n","\n","  img_show = img_p[0,:,:,0]\n","\n","  plt.imsave(\"/home/erik/Riksarkivet/Projects/handwritten-text-recognition/output/cycleGAN_models/overwritten/test_imgs/test1_%s.jpg\" % str(i).zfill(2), img_show, cmap='gray')\n","  print(i)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-07-15 09:08:21.596034: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-07-15 09:08:21.597288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n","2021-07-15 09:08:21.649362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n","pciBusID: 0000:02:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n","coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n","2021-07-15 09:08:21.649400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-15 09:08:21.651926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n","2021-07-15 09:08:21.652029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","2021-07-15 09:08:21.653004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n","2021-07-15 09:08:21.653286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n","2021-07-15 09:08:21.656194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n","2021-07-15 09:08:21.656879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n","2021-07-15 09:08:21.657092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n","2021-07-15 09:08:21.658458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n","2021-07-15 09:08:21.706744: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2021-07-15 09:08:21.707584: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-07-15 09:08:21.708257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n","pciBusID: 0000:02:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n","coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n","2021-07-15 09:08:21.708294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-15 09:08:21.708342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n","2021-07-15 09:08:21.708352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","2021-07-15 09:08:21.708361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n","2021-07-15 09:08:21.708370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n","2021-07-15 09:08:21.708379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n","2021-07-15 09:08:21.708388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n","2021-07-15 09:08:21.708397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n","2021-07-15 09:08:21.709302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n","2021-07-15 09:08:21.709325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-15 09:08:22.307202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-07-15 09:08:22.307225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n","2021-07-15 09:08:22.307230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n","2021-07-15 09:08:22.308641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:02:00.0, compute capability: 7.5)\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","2021-07-15 09:08:22.584221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n","2021-07-15 09:08:22.584685: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\n","2021-07-15 09:08:22.754917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n","2021-07-15 09:08:24.437760: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n","2021-07-15 09:08:24.910650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","hej\n","0\n","hej\n","1\n","hej\n","2\n","hej\n","3\n","hej\n","4\n","hej\n","5\n","hej\n","6\n"]}]},{"cell_type":"code","metadata":{"id":"faRlGhacv7et","executionInfo":{"status":"ok","timestamp":1622462093033,"user_tz":-120,"elapsed":565,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}}},"source":["import glob\n","from pathlib import Path\n","\n","imgs = glob.glob('/content/drive/MyDrive/Riksarkivet/htr/CycleGAN_models/Only_overwritten/TestCleaned/*.jpg')\n","\n","for img_path in imgs:\n","  img = cv2.imread(img_path)\n","  img = cv2.resize(img, (896,83))\n","  cv2.imwrite('/content/drive/MyDrive/Riksarkivet/htr/CycleGAN_models/Only_overwritten/after_cycleGAN/' + Path(img_path).name, img)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbH-2XM0gIv4"},"source":["img_trans = g_AB.predict(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEYteERAkFMQ"},"source":["img_trans = img_trans.reshape(-1, 128, 1024, 1)\n","img_show = img_trans[:,:,:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"GPU2ww6li6JV","executionInfo":{"status":"ok","timestamp":1622447749482,"user_tz":-120,"elapsed":383,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}},"outputId":"a66748e1-6a40-4061-96a4-2c804e58eb06"},"source":["from google.colab.patches import cv2_imshow\n","\n","open_cv_image = np.array(image_pil)\n","\n","cv2_imshow(open_cv_image)"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABAAAAACACAAAAACwiUSUAAAAlklEQVR4nO3BMQEAAADCoPVPbQ0PoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+DACeAAGpcCxCAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=1024x128 at 0x7F0150BAA7D0>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5OQQ_PGaKT_","executionInfo":{"status":"ok","timestamp":1622096612408,"user_tz":-120,"elapsed":6922,"user":{"displayName":"Erik Lenas","photoUrl":"","userId":"14196804914172684457"}},"outputId":"dc545b2e-a0c7-44a4-efa2-0360cd999b1b"},"source":["d_A = load_model('/content/drive/MyDrive/Riksarkivet/htr/CycleGAN_models/Only_overwritten/300_d_A.h5',custom_objects={'InstanceNormalization': InstanceNormalization()})\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"],"name":"stdout"}]}]}